Automated deployment for DeepCortex platform

If you plan to import existing IAM resources, you must have all IAM resources created ahead of time. Check the iam_resources.txt file to see what needs to be created.

If you will be running this in an environment without an internet connection you must place all necessary artifacts on S3 ahead of time and list this bucket in the main config file as the artifacts_s3_bucket.

The ami to be used for deployment should be RHEL based and have the following packages installed:

curl tar xz zip unzip ipset ntp cloud-init kernel-devel-$(uname -r) gettext

Running the entire deployment

1) Create a folder called environments on your computer.

2) Place the deepcortex-settings.tfvars file under this "environmnets" directory.

3) Fill in the necessary variables in deepcortex-settings.tfvars.

4) Have your custom endpoints.json file ready that will be used by the AWS CLI.

5) Export the following variables in your terminal. The this can be done by filling in envar.sh with the proper values and then sourcing this file from your terminal (source envar.sh).

CONFIG - the name of the config file to use (should be deepcortex-settings).
AWS_ACCESS_KEY_ID - the access key that should be used to deploy in AWS.
AWS_SECRET_ACCESS_KEY - the secret key that should be used to deploy in AWS.
CUSTOMER_KEY - the DC/OS enterprise key
DCOS_USERNAME - the username you'd like to use to login to the DC/OS cluster
DCOS_PASSWORD - the password you'd like to use to login to the DC/OS cluster (avoid special characters used in bash such as "#", ";", "$", etc.)
DOCKER_REGISTRY_AUTH_TOKEN - the token for docker authentication
DOCKER_EMAIL_LOGIN - the login email for docker
APPS_AWS_ACCESS_KEY_ID - the access key for the app user.
APPS_AWS_SECRET_ACCESS_KEY - the secret key for the app user.

6) Run "docker login -u falcondeepcortex" and enter the password: 6apXmvzquPKGDwzx.

7) Run "docker pull deepcortex/scorpius-deployment:falcon".

8) Run the following docker command replacing /path/to/environments with the path to the environments directory you created in step 1.

docker run \
  -v </path/to/environments>:/opt/deploy/environments \
  -v </path/to/endpoints.json>:/opt/deploy/ansible/common/files/extra_files \
  -e CONFIG=${CONFIG} \
  -e AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID} \
  -e AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY} \
  -e CUSTOMER_KEY=${CUSTOMER_KEY} \
  -e DCOS_USERNAME=${DCOS_USERNAME} \
  -e DCOS_PASSWORD=${DCOS_PASSWORD} \
  -e DOCKER_EMAIL_LOGIN=${DOCKER_EMAIL_LOGIN} \
  -e DOCKER_REGISTRY_AUTH_TOKEN=${DOCKER_REGISTRY_AUTH_TOKEN} \
  -e DOCKER_EMAIL_LOGIN=${DOCKER_EMAIL_LOGIN} \
  -e DOCKER_REGISTRY_AUTH_TOKEN=${DOCKER_REGISTRY_AUTH_TOKEN} \
  deepcortex/scorpius-deployment:falcon

9) Once your terminal output states the deployment is complete you can access the DeepCortex UI.

### Additional Docker Commands ###

Destroying the entire deployment
Everything, except the S3 buckets and any imported resources (e.g. VPC, IAM) will be destroyed.
By default, the S3 buckets are not destroyed, but can be manualy destroyed by passing the -t argument (see below).

docker run \
  -v ~/Desktop/environments:/opt/deploy/environments \
  -e CONFIG=${CONFIG} \
  -e AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID} \
  -e AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY} \
  -e CUSTOMER_KEY=${CUSTOMER_KEY} \
  -e DCOS_USERNAME=${DCOS_USERNAME} \
  -e DCOS_PASSWORD=${DCOS_PASSWORD} \
  -e DOCKER_EMAIL_LOGIN=${DOCKER_EMAIL_LOGIN} \
  -e DOCKER_REGISTRY_AUTH_TOKEN=${DOCKER_REGISTRY_AUTH_TOKEN} \
  -e DOCKER_EMAIL_LOGIN=${DOCKER_EMAIL_LOGIN} \
  -e DOCKER_REGISTRY_AUTH_TOKEN=${DOCKER_REGISTRY_AUTH_TOKEN} \
  -it --entrypoint "/bin/bash" deepcortex/scorpius-deployment:falcon /opt/deploy/destroy.sh

Building specific stacks (e.g to build just the DC/OS cluster supply "platform" as STACKS)

docker run \
  -v ~/Desktop/environments:/opt/deploy/environments \
  -v </path/to/endpoints.json>:/opt/deploy/ansible/common/files/extra_files \
  -e CONFIG=${CONFIG} \
  -e AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID} \
  -e AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY} \
  -e CUSTOMER_KEY=${CUSTOMER_KEY} \
  -e DCOS_USERNAME=${DCOS_USERNAME} \
  -e DCOS_PASSWORD=${DCOS_PASSWORD} \
  -e DOCKER_EMAIL_LOGIN=${DOCKER_EMAIL_LOGIN} \
  -e DOCKER_REGISTRY_AUTH_TOKEN=${DOCKER_REGISTRY_AUTH_TOKEN} \
  -e DOCKER_EMAIL_LOGIN=${DOCKER_EMAIL_LOGIN} \
  -e DOCKER_REGISTRY_AUTH_TOKEN=${DOCKER_REGISTRY_AUTH_TOKEN} \
  -it --entrypoint "/bin/bash" deepcortex/scorpius-deployment:falcon /opt/deploy/build.sh -s STACKS

all optional arguments:
  -b: shutdown boostrap - can be set to true destroy bootstrap node after the cluster deploys
  -g: gpu on start - can be set to false to exclude spinning up a gpu node after the cluster deploys
  -m: deploy mode - can be set to simple to exclude download of DC/OS cli and extra output
  -s: stacks - a list of comma separated values to overwrite which terraform stacks to build
  -p: packer - can be set to false to exclude packer builds
  -t: s3 type - can be set to existing to import existing S3 buckets

Destroying specific stacks (e.g to destroy just the DC/OS cluster supply "platform" as STACKS)

docker run \
  -v ~/Desktop/environments:/opt/deploy/environments \
  -e CONFIG=${CONFIG} \
  -e AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID} \
  -e AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY} \
  -e CUSTOMER_KEY=${CUSTOMER_KEY} \
  -e DCOS_USERNAME=${DCOS_USERNAME} \
  -e DCOS_PASSWORD=${DCOS_PASSWORD} \
  -e DOCKER_EMAIL_LOGIN=${DOCKER_EMAIL_LOGIN} \
  -e DOCKER_REGISTRY_AUTH_TOKEN=${DOCKER_REGISTRY_AUTH_TOKEN} \
  -e DOCKER_EMAIL_LOGIN=${DOCKER_EMAIL_LOGIN} \
  -e DOCKER_REGISTRY_AUTH_TOKEN=${DOCKER_REGISTRY_AUTH_TOKEN} \
  -it --entrypoint "/bin/bash" deepcortex/scorpius-deployment:falcon /opt/deploy/destroy.sh -s STACKS

all optional arguments:
  -s: stacks - a list of comma separated values to overwrite which terraform stacks to destroy
  -d: can be set to true to delete s3 buckets


